<script>
/* =========================================================
   HANDS-FREE VOICE LOOP (Enable once, then it runs itself)
   - Enable Voice: requests mic permission + checks support
   - Auto listen loop: listen -> auto-send -> speak -> listen
   - Clears transcript every cycle (no retained text)
   - No "press talk every time"
   ========================================================= */

const CCare = (() => {
  // ---------- CONFIG ----------
  const CFG = {
    lang: "en-AU",
    autoRestartDelayMs: 250,    // restart listen after speaking
    endSilenceMs: 800,          // if your browser supports it (Chrome flags)
    speakReplies: true,         // set false if you only want text replies
    debug: false,
  };

  // ---------- STATE ----------
  const state = {
    mode: "idle",               // idle | permission | ready | listening | thinking | speaking | error
    voiceEnabled: false,
    handsFree: true,
    lastTranscript: "",
    isUserSpeaking: false,
    abortCtrl: null,
    recognition: null,
    micStream: null,
    speaking: false,
    stopAll: false,
  };

  // ---------- HOOK THESE TO YOUR UI ----------
  // Replace these with your actual DOM functions
  function uiStatus(text) { if (CFG.debug) console.log("[status]", text); }
  function uiSystem(text) { console.log("[system]", text); }
  function uiUser(text)   { console.log("[you]", text); }
  function uiAI(text)     { console.log("[ai]", text); }

  function setMode(mode, err) {
    state.mode = mode;
    if (err) state.error = err;
    uiStatus(`${mode}${err ? ": " + err : ""}`);
  }

  // ---------- SUPPORT CHECK ----------
  function supported() {
    return !!(window.SpeechRecognition || window.webkitSpeechRecognition) &&
           !!navigator.mediaDevices?.getUserMedia;
  }

  // ---------- ENABLE VOICE (ONE TIME) ----------
  async function enableVoiceOnce() {
    state.stopAll = false;
    setMode("permission");

    if (!supported()) {
      state.voiceEnabled = false;
      setMode("error", "Browser does not support speech recognition.");
      uiSystem("Voice unavailable on this browser.");
      return false;
    }

    // Ask permission first
    try {
      state.micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    } catch (e) {
      state.voiceEnabled = false;
      setMode("error", "Microphone permission denied.");
      uiSystem("Microphone permission denied.");
      return false;
    }

    // Build recognition
    const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
    const rec = new SR();
    rec.lang = CFG.lang;
    rec.interimResults = true;      // we want real-time, but only send final
    rec.continuous = true;          // keep listening
    // Some Chrome versions support these:
    try { rec.maxAlternatives = 1; } catch {}
    // Try to reduce "retained" phrases by resetting on each cycle (we do anyway)

    rec.onstart = () => {
      state.isUserSpeaking = false;
      setMode("listening");
    };

    rec.onresult = (e) => {
      // Build current utterance
      let interim = "";
      let finalText = "";

      for (let i = e.resultIndex; i < e.results.length; i++) {
        const r = e.results[i];
        const t = (r[0]?.transcript || "").trim();
        if (!t) continue;
        if (r.isFinal) finalText += (finalText ? " " : "") + t;
        else interim += (interim ? " " : "") + t;
      }

      // Show "live" listening text if you want (optional)
      // IMPORTANT: We do NOT retain it for next turn.
      if (interim) {
        state.lastTranscript = interim;
        state.isUserSpeaking = true;
      }

      if (finalText) {
        // Clear buffer immediately so it doesn't "retain"
        state.lastTranscript = "";
        state.isUserSpeaking = false;

        // Stop recognition so we can send + speak without it hearing itself
        safeStopRecognition();

        // Send this finalText automatically
        handleFinalUtterance(finalText);
      }
    };

    rec.onerror = (e) => {
      const msg = e?.error || "speech_error";
      // Common: "no-speech" / "aborted" / "audio-capture" / "not-allowed"
      if (state.stopAll) return;

      // If it aborted because we stopped it intentionally, ignore
      if (msg === "aborted") return;

      setMode("error", msg);
      uiSystem("Voice error: " + msg);

      // Auto-recover
      if (state.voiceEnabled && state.handsFree) {
        setTimeout(() => safeStartRecognition(), 500);
      }
    };

    rec.onend = () => {
      if (state.stopAll) return;
      // If hands-free and not speaking/thinking, restart listening
      if (state.voiceEnabled && state.handsFree && state.mode !== "thinking" && state.mode !== "speaking") {
        setTimeout(() => safeStartRecognition(), 250);
      }
    };

    state.recognition = rec;
    state.voiceEnabled = true;
    setMode("ready");
    uiSystem("Voice enabled. Hands-free listening is ON.");
    return true;
  }

  // ---------- START/STOP RECOGNITION SAFELY ----------
  function safeStartRecognition() {
    if (!state.recognition || state.stopAll) return;
    if (state.mode === "speaking" || state.mode === "thinking") return;

    try { state.recognition.start(); } catch (e) {
      // If already started, ignore
    }
  }

  function safeStopRecognition() {
    if (!state.recognition) return;
    try { state.recognition.onresult = state.recognition.onresult; } catch {}
    try { state.recognition.stop(); } catch {}
  }

  // ---------- SEND TO CHAT API ----------
  async function callChatApi(userText) {
    // Abort previous
    if (state.abortCtrl) state.abortCtrl.abort();
    state.abortCtrl = new AbortController();

    // IMPORTANT:
    // If your backend isn't actually working yet, this will fail.
    // But the hands-free voice loop logic is correct.
    const res = await fetch("/api/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      signal: state.abortCtrl.signal,
      body: JSON.stringify({ text: userText })
    });

    if (!res.ok) {
      const t = await res.text().catch(() => "");
      throw new Error("API error: " + res.status + " " + t);
    }

    const data = await res.json();
    return data.reply || data.text || "";
  }

  async function handleFinalUtterance(finalText) {
    uiUser(finalText);
    setMode("thinking");

    let reply = "";
    try {
      reply = await callChatApi(finalText);
    } catch (e) {
      setMode("error", e.message || "chat_error");
      uiSystem("Chat failed: " + (e.message || "unknown"));
      // After error, go back to listening
      setTimeout(() => {
        setMode("ready");
        safeStartRecognition();
      }, 500);
      return;
    }

    uiAI(reply || "(no reply)");

    if (CFG.speakReplies && reply) {
      await speak(reply);
    }

    // Loop back to listening automatically
    setMode("ready");
    if (state.voiceEnabled && state.handsFree) {
      setTimeout(() => safeStartRecognition(), CFG.autoRestartDelayMs);
    }
  }

  // ---------- TTS (optional) ----------
  function speak(text) {
    return new Promise((resolve) => {
      if (!CFG.speakReplies || !("speechSynthesis" in window)) return resolve();

      state.speaking = true;
      setMode("speaking");

      // Cancel anything queued
      try { window.speechSynthesis.cancel(); } catch {}

      const u = new SpeechSynthesisUtterance(text);
      u.lang = CFG.lang;

      u.onend = () => { state.speaking = false; resolve(); };
      u.onerror = () => { state.speaking = false; resolve(); };

      // Prevent recognition from hearing itself
      safeStopRecognition();

      window.speechSynthesis.speak(u);
    });
  }

  // ---------- PUBLIC CONTROLS ----------
  async function enableVoiceAndStart() {
    const ok = await enableVoiceOnce();
    if (ok) safeStartRecognition();
  }

  function stopAll() {
    state.stopAll = true;
    state.voiceEnabled = false;
    try { window.speechSynthesis?.cancel?.(); } catch {}
    safeStopRecognition();
    if (state.micStream) {
      state.micStream.getTracks().forEach(t => t.stop());
      state.micStream = null;
    }
    setMode("idle");
    uiSystem("Voice stopped.");
  }

  return {
    enableVoiceAndStart,
    stopAll,
    // optional toggles:
    setSpeakReplies(v){ CFG.speakReplies = !!v; },
    setDebug(v){ CFG.debug = !!v; },
  };
})();

/* =========================================================
   BUTTON WIRING
   Replace these IDs with your actual button IDs.
   - Enable Voice button should call enableVoiceAndStart()
   - Talk button can be removed or hidden (not needed)
   ========================================================= */

document.getElementById("enableVoiceBtn")?.addEventListener("click", () => {
  CCare.enableVoiceAndStart();
});

// Optional stop button
document.getElementById("stopVoiceBtn")?.addEventListener("click", () => {
  CCare.stopAll();
});
</script>
